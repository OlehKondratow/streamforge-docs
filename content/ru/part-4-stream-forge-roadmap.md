+++
date = '2025-08-25T10:04:00+02:00'
draft = false
title = 'Приложения и технические детали'
weight = 4
+++

# Часть V: Приложения

### **Приложение А: Схемы данных и API**

*(Этот раздел будет содержать полную спецификацию OpenAPI для `queue-manager`, а также детальные JSON-схемы для всех сообщений, передаваемых через Kafka.)*

### **Приложение Б: Примеры манифестов Kubernetes**

#### **Пример: Kubernetes Job для `arango-candles`**

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-arango-candles-btcusdt-abc123
  namespace: stf
  labels:
    app: streamforge
    queue_id: "wf-btcusdt-api_candles_1m-20240801-abc123"
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: arango-candles
          image: registry.dmz.home/streamforge/arango-candles:v0.1.5
          env:
            - name: QUEUE_ID
              value: "wf-btcusdt-api_candles_1m-20240801-abc123"
            - name: SYMBOL
              value: "BTCUSDT"
            - name: TYPE
              value: "api_candles_1m"
            - name: KAFKA_TOPIC
              value: "wf-btcusdt-api_candles_1m-20240801-abc123-data"
            - name: COLLECTION_NAME
              value: "btcusdt_api_candles_1m_2024_08_01"
            # ... прочие переменные из ConfigMap и Secret ...
      nodeSelector:
        streamforge-worker: "true" # Пример селектора для выделенных нод
  backoffLimit: 2
  ttlSecondsAfterFinished: 3600
```

### **Приложение В: Примеры CI/CD пайплайнов**

*(Этот раздел будет содержать полные `.gitlab-ci.yml` файлы для каждого микросервиса, демонстрирующие этапы тестирования, сборки и деплоя.)*

### **Приложение Г: Глоссарий терминов**

*(Этот раздел будет содержать определения ключевых терминов, используемых в проекте: Workflow, Job, Декаплинг, Идемпотентность и т.д.)*

### **Приложение Д: Руководство по развертыванию и эксплуатации**

*(Этот раздел будет содержать пошаговые инструкции для развертывания всей платформы с нуля, а также руководство по основным операционным процедурам: мониторинг, бэкапы, обновление компонентов.)*

### **Приложение E: Описание модулей**

*   **core-modules/argocd/**: Управляет развертыванием ArgoCD, инструмента для реализации GitOps, который обеспечивает декларативное и непрерывное развертывание приложений в Kubernetes.
*   **core-modules/cert-manager/**: Отвечает за автоматизацию управления TLS-сертификатами в Kubernetes, включая их выпуск и ротацию.
*   **core-modules/gitlab-runner/**: Содержит конфигурацию для регистрации и запуска GitLab Runner в Kubernetes, что позволяет выполнять CI/CD-пайплайны.
*   **core-modules/postgres-operator/**: Управляет развертыванием и жизненным циклом кластеров PostgreSQL с использованием оператора Zalando.
*   **core-modules/elk3/**: Содержит конфигурационные файлы для развертывания стека ELK (Elasticsearch, Logstash, Kibana) для сбора, хранения и анализа логов.
*   **core-modules/GlusterFS/**: Обеспечивает настройку распределенной файловой системы GlusterFS для предоставления постоянных томов с доступом ReadWriteMany (RWX).
*   **core-modules/gpu/**: Управляет поддержкой GPU-ускорителей в Kubernetes, включая установку плагинов устройств NVIDIA.
*   **core-modules/jupyter/**: Содержит конфигурацию для развертывания JupyterHub, многопользовательской платформы для запуска интерактивных ноутбуков.
*   **core-modules/kafka/**: Отвечает за развертывание кластера Apache Kafka с использованием оператора Strimzi.
*   **core-modules/kafka-ui/**: Управляет развертыванием Kafka UI, веб-интерфейса для управления и мониторинга кластеров Kafka.
    *   **Версия:** `v0.7.2` (образ `provectuslabs/kafka-ui:v0.7.2`)
    *   **Доступ:** Доступен через Ingress по адресу `kafka-ui.dmz.home` с использованием `nginx` Ingress-контроллера и TLS-сертификата (`kafka-ui-tls`).
    *   **Размещение подов:** Настроен на запуск на узле `k2w-7` с помощью `nodeSelector`.
    *   **Реплики:** Развернут в одном экземпляре (`replicaCount: 1`).
    *   **Подключение к Kafka:** Подключается к Kafka кластеру `k3` по адресу `k3-kafka-bootstrap.kafka:9093` с использованием `SASL_SSL` и механизма аутентификации `SCRAM-SHA-512` с пользователем `user-streamforge`.
*   **core-modules/keycloak/**: Содержит конфигурацию для развертывания Keycloak, сервера для управления идентификацией и доступом.
*   **core-modules/kubernetes/**: Содержит базовые конфигурационные файлы для настройки кластера Kubernetes.
*   **core-modules/metallb/**: Обеспечивает реализацию балансировщика нагрузки (LoadBalancer) для bare-metal кластеров Kubernetes.

### **Приложение F: Процедура тестирования**

Для проверки работоспособности системы и тестирования сквозного прохождения команд и событий через Kafka используются `dummy-service` и `debug_producer.py`. Эти инструменты особенно эффективны в стандартизированной среде разработки `devcontainer`.

**1. `dummy-service`: Тестовый микросервис для симуляции**

`dummy-service` — это тестовый микросервис StreamForge, предназначенный для симуляции поведения других сервисов, тестирования связности Kafka и отладки. Он может получать команды из `queue-control`, отправлять события в `queue-events`, имитировать загрузку и ошибки, а также публиковать Prometheus-метрики.

*   **Запуск:** Запустите `dummy-service` в Kubernetes как `Job` или `Pod`, передав ему необходимые переменные окружения. Для локального тестирования в `devcontainer` используйте:
    ```bash
    python3.11 -m app.main --debug --simulate-loading
    ```
    (без `--exit-after`, чтобы сервис работал постоянно для интерактивного тестирования).
*   **Подробнее:** См. `services/dummy-service/README.md`.

**2. `debug_producer.py`: Инструмент для отправки команд и проверки ответов**

`debug_producer.py` — это CLI-инструмент для отправки тестовых команд (`ping`, `stop`) в Kafka-топик `queue-control` и ожидания ответов (`pong`) из `queue-events`. Он используется для отладки и тестирования микросервисов, взаимодействующих через Kafka.

*   **Тестирование связности Kafka (ping/pong):**
    Отправьте команду `ping` и ожидайте `pong` для проверки базовой связности и работоспособности целевого микросервиса.
    ```bash
    python3.11 services/dummy-service/debug_producer.py \
      --queue-id <your-queue-id> \
      --command ping \
      --expect-pong
    ```
    `debug_producer.py` будет ожидать ответа `pong` от `dummy-service` (или другого сервиса, который обрабатывает `ping`) и выведет время приема-передачи (RTT). Это подтверждает, что `dummy-service` получил команду, обработал ее и отправил ответное событие в Kafka.

*   **Тестирование команды остановки (stop):**
    Отправьте сигнал `stop` целевому сервису, который должен корректно завершить свою работу.
    ```bash
    python3.11 services/dummy-service/debug_producer.py \
      --queue-id <your-queue-id> \
      --command stop
    ```

*   **Тестирование имитации загрузки и отслеживания статуса:**
    Запустите `dummy-service` с флагом `--simulate-loading`. Используйте `debug_producer.py` (или другой потребитель) для отслеживания событий `loading`, которые `dummy-service` отправляет в `queue-events`.

*   **Тестирование симуляции сбоя:**
    Запустите `dummy-service` с флагом `--fail-after N`. Наблюдайте за логами `dummy-service` и событиями в `queue-events`, чтобы убедиться, что сервис корректно отправляет событие `error` перед завершением.

*   **Тестирование Prometheus-метрик:**
    Запустите `dummy-service` и используйте `curl localhost:8000/metrics` для проверки экспортируемых метрик. Отправляйте команды с `debug_producer.py` и наблюдайте за обновлением метрик.

*   **Подробнее:** См. `services/dummy-service/debug_producer.md`.

**3. Использование `devcontainer` для тестирования:**

Среда `devcontainer` предоставляет стандартизированное и изолированное окружение, которое имитирует продакшн-среду (с доступом к Kubernetes, Kafka и т.д.). Это делает тесты, выполняемые в `devcontainer`, особенно ценными, так как они максимально приближены к реальным условиям развертывания. Все вышеописанные тесты рекомендуется выполнять именно в этой среде.

### **Приложение G: Управление ресурсами Kafka**

Каталог `cred-kafka-yaml/` содержит манифесты Kubernetes для декларативного управления ресурсами Kafka с помощью оператора Strimzi. Эти манифесты включают:

*   **Определения топиков:** Создание управляющих топиков, таких как `queue-control` и `queue-events`.
*   **Пользователи и права доступа:** Создание пользователей Kafka (например, `user-streamforge`) и настройка их прав доступа (ACL).
*   **Секреты:** Хранение учетных данных для доступа к Kafka.

### **Приложение H: Среда отладки в Kubernetes**

Для отладки и взаимодействия с кластером Kubernetes в проекте предусмотрены следующие среды:

*   **JupyterHub:** Позволяет запускать интерактивные сессии Jupyter Notebook прямо в кластере, предоставляя аналитикам и разработчикам готовую среду для взаимодействия с Kubernetes API и данными.

    **Ключевые особенности конфигурации:**
    *   **Управление неактивными серверами (Culling):** Включено автоматическое завершение неактивных серверов Jupyter. Серверы администраторов завершаются через 7200 секунд (2 часа) без активности, проверка происходит каждые 600 секунд (10 минут). Пользовательские серверы не завершаются автоматически.
    *   **Аутентификация:** Используется простая "фиктивная" (dummy) аутентификация, что упрощает доступ в тестовой среде.
    *   **База данных Hub:** Используется `sqlite-memory` для базы данных JupyterHub, но она персистентно монтируется с хоста (`/data/home/jovyan/hub-db`) для сохранения состояния.
    *   **Размещение подов:** Как Hub, так и пользовательские серверы (`singleuser` pods) настроены на запуск на конкретном узле (`k2w-8`) с помощью `nodeSelector`.
    *   **Доступ:** Доступ к JupyterHub осуществляется через Ingress по адресу `jupyterhub.dmz.home` с использованием `Traefik` Ingress-контроллера и TLS-сертификата (`jupyterhub-tls`).
    *   **Образы пользовательских серверов:** Используется кастомный образ `registry.dmz.home/streamforge/core-modules/jupyter-python311:v0.0.2`, который включает `kubectl`, `helm` и другие необходимые инструменты.
    *   **Ресурсы пользовательских серверов:** Гарантированный объем памяти для каждого пользовательского сервера составляет `1G`.
    *   **Хранилище пользовательских данных:** Для пользовательских данных и проектов используются персистентные тома, монтируемые с хоста:
        *   `/home/` (GlusterFS `gf-home`)
        *   `/data/project` (GlusterFS `gf-projects`)
        *   `/data/venv` (GlusterFS `gf-venv`)
    *   **Безопасность пользовательских подов:** Пользовательские поды запускаются с `UID: 1001` и `FSGID: 100`.
    *   **Интеграция с Docker Registry:** Используется секрет `regcred` для аутентификации при скачивании образов.

*   **Dev Container (VS Code):**
    Представляет собой Docker-контейнер, который служит полноценной средой разработки, интегрированной с VS Code. Он обеспечивает стандартизированное и изолированное окружение для всех разработчиков, устраняя проблемы "работает у меня на машине".

    **Ключевые особенности:**
    *   **Базовый образ:** Ubuntu 22.04 LTS.
    *   **Предустановленные инструменты:** `kubectl`, Helm, `gitlab-runner`, `git`, `curl`, `ssh` и другие стандартные утилиты.
    *   **Настройка доступа к Kubernetes:** Автоматически конфигурирует переменные окружения для `kubectl`, позволяя взаимодействовать с кластером Kubernetes.
    *   **Управление пользователями и SSH:** Создает отдельного пользователя и настраивает SSH-доступ.
    *   **Управление сертификатами:** Устанавливает CA-сертификаты для доверия к внутренним сервисам с TLS.

    **Как использовать:**
    1.  Установите Docker Desktop (или Docker Engine) и расширение "Dev Containers" для VS Code.
    2.  Откройте проект StreamForge в VS Code и выберите "Reopen in Container" (или "Open Folder in Container").
    3.  VS Code соберет Docker-образ на основе `platform/devcontainer/Dockerfile` и запустит контейнер, предоставляя готовую среду для разработки.

*   **Dev-контейнер (общий):** Представляет собой Docker-образ с широким набором инструментов для разработки и отладки, включая `kubectl`, `helm`, `kafkacat` и `python`. Этот образ можно использовать для запуска временных подов в Kubernetes, которые служат в качестве полноценной интерактивной среды для отладки микросервисов.
