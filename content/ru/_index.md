+++
date = '2025-08-25T13:00:00+02:00'
draft = false
title = 'Архитектурный манифест StreamForge: почему мы ставим всё на асинхронность'
weight = 1
+++

### Коллеги,

Этот документ — наш архитектурный манифест. Его цель — не просто описать, а **обосновать** фундаментальный выбор, который определяет всю нашу инженерную культуру: **отказ от синхронного взаимодействия в пользу 100% асинхронной, событийно-ориентированной модели**.

Я хочу детально, на уровне принципов и практических последствий, показать, почему классический подход с прямыми API-вызовами для нашего домена — это путь к созданию хрупкого, немасштабируемого и сложного в поддержке "распределенного монолита". И почему **Apache Kafka** в роли "центральной нервной системы" — это не просто модная технология, а стратегическая инвестиция в **надежность, масштабируемость и скорость разработки** на годы вперед.

---

### Часть 1: Анатомия боли. Глубокие проблемы синхронных систем.

Давайте на чистоту. Построить систему на прямых API-вызовах (будь то REST или gRPC) — это самый очевидный и быстрый путь для старта. Сервис А вызывает сервис Б. Просто и понятно. Но для нашего домена — это архитектурный тупик.

Крипто-рынок — это идеальный шторм: волатильность 24/7, гигантские объемы данных и непредсказуемые пиковые нагрузки. Попытка построить здесь что-то на синхронных вызовах неизбежно приводит к следующим проблемам:

1.  **Каскадные сбои (Cascading Failures):** Представим цепочку: `loader` (принимает данные с биржи) -> `parser` (приводит их к нашему формату) -> `writer` (пишет в БД). Если `writer` начинает тормозить из-за нагрузки на базу или уходит в рестарт, он создает **обратное давление (backpressure)**. `parser` ждет, у него переполняются буферы. Он перестает принимать данные от `loader`. `loader` тоже вынужден останавливаться. В итоге, минутный сбой на уровне базы данных приводит к полной остановке приема данных и их потере. Мы строим "карточный домик".

2.  **"Болтливые" микросервисы и "распределенный монолит":** В синхронной модели сервисы вынуждены постоянно "болтать" друг с другом. Это создает жесткие, неявные зависимости. Сервис А должен знать адрес сервиса Б. Он должен знать его API, обрабатывать его специфические ошибки. Со временем это превращается в "распределенный монолит" — менять что-то в одном сервисе страшно, потому что непонятно, где и как это аукнется.

3.  **Проблема масштабирования:** Как масштабировать такую систему? Если `parser` стал узким местом, мы можем запустить 10 его экземпляров. Но теперь `loader` должен знать, как балансировать нагрузку между ними? А что если один из `parser`-ов умрет? `loader` должен реализовать логику повторных запросов (retry) и переключения на живой экземпляр. Мы начинаем дублировать сложную логику отказоустойчивости в каждом сервисе.

---

### Часть 2: Наша ставка — персистентный лог как сердце системы.

Осознав это, мы сделали стратегическую ставку на **событийно-ориентированную архитектуру (EDA)**. Но не просто на "очередь сообщений", а на **Apache Kafka** как реализацию паттерна "распределенный персистентный лог".

**В чем ключевое отличие от условного RabbitMQ?**
-   **RabbitMQ (и похожие брокеры)** — это, по сути, почтальон. Он взял сообщение, доставил первому получателю, и сообщение исчезло. Это отлично подходит для распределения задач, но не для потоковой обработки данных.
-   **Kafka** — это скорее "центральный архив" или "бортовой журнал" всей компании. Каждое событие ("сделка по BTCUSDT в 12:05:03") записывается в этот журнал (топик) и остается там на заданное время (например, 7 дней), даже после того, как его прочитали.

Это фундаментальное отличие меняет все. Наши сервисы теперь не общаются друг с другом. Они взаимодействуют с этим центральным архивом:
-   **Продюсеры** просто дописывают новые события в конец журнала. Их не волнует, кто, когда и сколько раз прочитает это событие.
-   **Консьюмеры** читают этот журнал, каждый в своем темпе. Kafka для каждого консьюмера (точнее, для `consumer group`) помнит, на каком месте в журнале он остановился (это называется `offset`).

#### 2.1. Анатомия Kafka: Партиции, Ключи, Оффсеты и Консьюмер-группы

Чтобы понять всю мощь, нужно разобраться в четырех концепциях:
-   **Партиции:** Топик в Kafka — это не один большой лог, а набор из N параллельных, упорядоченных логов, называемых партициями. **Партиция — это единица параллелизма.**
-   **Ключи:** Когда продюсер отправляет событие, он может указать ключ (например, `symbol="BTCUSDT"`). Kafka гарантирует, что все события с одним и тем же ключом всегда попадут в одну и ту же партицию. Это **сохраняет порядок событий в рамках одной сущности** (все сделки по BTCUSDT будут идти строго друг за другом).
-   **Оффсеты:** Каждый потребитель сам отвечает за то, какое последнее сообщение он прочитал в каждой партиции. Номер этого сообщения (`offset`) он периодически сохраняет обратно в Kafka. Это дает потребителям полный контроль над процессом чтения.
-   **Консьюмер-группы:** Несколько инстансов одного сервиса (например, 5 подов `arango-connector`) могут объединиться в одну консьюмер-группу (указав одинаковый `group.id`). Kafka автоматически распределит все партиции топика между этими 5 инстансами. Если один из них упадет, Kafka за доли секунды **перераспределит (rebalance)** его партиции между оставшимися. Это и есть механизм отказоустойчивости и масштабирования.

---

### Часть 3: Суперсилы, которые мы получаем.

Эта архитектура дает нам три стратегических преимущества, недостижимых в синхронном мире.

#### 3.1. Абсолютная отказоустойчивость и самовосстановление

Kafka выступает как гигантский амортизатор между сервисами.
**Пример:** Наш сервис `gnn-trainer` (обучение графовых моделей) — тяжелый и ресурсоемкий. Он читает данные из топика `graph-features`. Допустим, мы хотим развернуть его новую версию. Мы просто останавливаем старую, разворачиваем новую. На это уходит 5 минут. Все эти 5 минут сервис `graph-builder` продолжает спокойно работать и публиковать новые фичи для графов в топик. Они там копятся. Когда `gnn-trainer` стартует, он видит свой старый `offset`, понимает, что отстал, и начинает в ускоренном темпе обрабатывать накопившиеся данные. **С точки зрения системы в целом, сбоя не было. Была задержка в обработке, но не потеря данных или остановка.**

#### 3.2. Эластичность и линейное масштабирование

Это больше не теория, а операционная реальность.
**Пример:** Наш топик `raw-orderbooks` имеет 64 партиции. В спокойное время его обрабатывают 8 инстансов `orderbook-processor`. Каждый обрабатывает по 8 партиций. Начинается выступление главы ФРС, волатильность зашкаливает, поток данных вырастает в 20 раз. Наш дашборд в Grafana показывает, что `consumer lag` (отставание потребителей) растет.
**Наши действия:** `kubectl scale deployment/orderbook-processor --replicas=64`.
**Что происходит дальше:** Kubernetes создает 56 новых подов. Они присоединяются к той же консьюмер-группе. Kafka запускает ребалансировку, и через несколько секунд у нас уже 64 инстанса, каждый из которых обрабатывает одну партицию. Пропускная способность обработки вырастает в 8 раз. Когда нагрузка спадет, мы так же легко вернем количество реплик к 8. **Мы перешли от сложной инженерной проблемы производительности к тривиальной операционной задаче.**

#### 3.3. Эволюция архитектуры и концепция "Data Mesh"

Это самый мощный стратегический плюс. Мы превращаем наши потоки данных в **"продукты"**.
Команда, отвечающая за загрузчики, владеет "продуктом" — топиком `raw-trades`. Их обязанность — поставлять в этот топик качественные, валидные данные с определенным SLA.
-   Завтра команда **Data Science** захочет построить модель предсказания волатильности. Им не нужно идти к команде загрузчиков. Они просто создают свой сервис `volatility-predictor` с новой `consumer group` и начинают читать данные из `raw-trades`.
-   Послезавтра команда **Security** захочет внедрить систему фрод-мониторинга. Они создают свой сервис `fraud-detector` и подписываются на тот же самый топик.

Все эти команды работают **параллельно и независимо**. Они не могут помешать друг другу или сломать основной конвейер данных. Это позволяет нам развивать продукт с невиданной скоростью и гибкостью. Мы реализуем на практике концепцию **Data Mesh**, где данные становятся децентрализованным, легкодоступным и надежным ресурсом для всей компании.

---

### Итог: Мы инвестируем в скорость.

Переход на EDA и Kafka — это не усложнение ради усложнения. Это осознанный трейд-офф. Мы принимаем чуть большую сложность на старте, чтобы получить фундаментальные преимущества в будущем.

Мы строим платформу, которая дает нам:
-   **Надежность**, заложенную в саму архитектуру.
-   **Масштабируемость**, ограниченную только ресурсами нашего кластера, а не дизайном софта.
-   **Скорость и гибкость** в разработке, позволяющую нам быстро проверять гипотезы и выводить на рынок новые продукты.

Это наш фундамент. И он — железобетонный.